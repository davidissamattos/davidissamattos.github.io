<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Item Response Theory on David's public blog</title><link>https://davidissamattos.github.io/tags/item-response-theory/</link><description>Recent content in Item Response Theory on David's public blog</description><generator>Hugo</generator><language>en</language><lastBuildDate>Thu, 15 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://davidissamattos.github.io/tags/item-response-theory/index.xml" rel="self" type="application/rss+xml"/><item><title>On the Assessment of Benchmark Suites for Algorithm Comparison</title><link>https://davidissamattos.github.io/post/2021/evo-irt-paper/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://davidissamattos.github.io/post/2021/evo-irt-paper/</guid><description>&lt;p&gt;Today, we submitted a new paper (On the Assessment of Benchmark Suites for Algorithm Comparison) to the IEEE Transactions on Evolutionary Computing (and of course to Arxiv). In this paper, we challenge the current state-of-the-art suites used for benchmarking in evolutionary computing.&lt;/p&gt;
&lt;p&gt;Specifically, we utilize a Bayesian Item Response Theory model combined with Fisher Information to analyze how two famous benchmark suites (the BBOB and the PBO) actually perfom in practice in terms of the empirical succcess rate. We found that there is a big mismatch between the difficulty levels of these suites and the state-of-the-art algorithms. This leads to benchmark suites that have low discrimination and that are not good at estimating the ability of algorithms to solve a problem.&lt;/p&gt;</description></item></channel></rss>